# CountingBloomFilters

## Introduction to the data structure

 **Counting Bloom Filter(CBF)** is a probabilistic data structure that efficiently checks whether an element is in a set. The idea behind the CBF is to have a bit array as an underlying data structure, where all the bits are initially set to 0. CBF has a set of hash functions that will map our item to a set of positions in the bit array. A single item can be mapped only to a single possible set of bits; however, two different items can be mapped to a single set of the positions in the bit array (this event is called a collision). 
  
  As mentioned before, CBFs help us check whether an element is in the set. As briefly mentioned, the first operation supported by CBF is _hashing_ – mapping our item to the set of indexes in the bit array with the set of hash functions. The number of indexes the hashing method will produce equals the number of the hash functions since each function will produce a separate index in the bit array for every item hashed. In its classical implementation, if we assume that hash functions are efficient (meaning that the runtime of calculating a hash value won't exceed a particular number), each hash function would have O(1) time complexity. The insert method, hence, will have an O(1) time complexity since the number of hash functions is constant and predefined before any operations with CBF.

  Let's define how we add an element to the set (_insert_ method). As mentioned previously, each hash function (let's define that we have k hash functions within our CBF) would produce a position in the bit array to which our element is mapped. The first part of this method is applying the hashing method to an item we want to insert. Each of the k counters in the bit array we will get should be increased by 1. The time complexity of this method will also be O(1) since it consists of the hashing method (which is O(1)), and a constant number of elements increases in the bit array (each of them also has O(1) time complexity).

  Our next method is _searching_ for an element in the set, the main objective of Counting Bloom Filters. To efficiently search for an element in the set, we must apply the hash method first. After obtaining k (total number of hash functions) positions, we need to check their values in the bit array. If one of these k indexes corresponds to a 0 in our bit array, our value is certainly not in the set. Why? Let's assume the opposite. When an element was added to the set, corresponding k positions in the bit array were increased by 1, meaning that none of them could have been 0 on the stage of searching for an element. This property is supported by the fact that our hash functions would be deterministic – a specific input could only produce one output. This means that the k positions created by hash functions on the insertion stage will be the same during hashing on the search stage, eliminating the possibility of a False Negative (in other words, if an element was added to the set, the search method will 100% tell us so).
On the other hand, what if all k of our indexes are greater than 0? Does it mean that our element is present in the set? The answer is: likely, but not necessarily. As we mentioned before, the possibility of a False Negative is 0 because all the hash indexes will be greater than 0 if our element is added. On the other hand, these indexes could have been increased by the presence of other elements in the set because our hash functions can produce the same output for different inputs, "fooling" us into believing that the element is in the set. This is why Counting Bloom Filters is called a probabilistic data structure – possible answers for the search method are "No, an element is certainly not in the set" or "The element is likely to be in the set." To increase the likelihood of an element being in the set if the second answer occurs, we need to decrease the possibility of a False Positive (when the program tells us an element is in the set while it is actually not). To accomplish this, we will ensure that the hash functions we choose are deterministic (mentioned previously), have a uniform distribution of values, and are independent (will be discussed in more detail later). The time complexity of this method is also O(1) since it consists of a hash method (which is O(1)) and a limited number of "checks" in the bit array (each of them is also O(1)).

  The last method CBFs support is a _delete_ method. If we wish to delete an element from the set, we first apply the search method (defined earlier) to verify whether an element is in the set. If true, the hashing method will determine k spots in the bit array that will correspond to our element, and each of the counters will be reduced by one. If the search method shows that our element is not in the set, we won't change any of the counters and raise an error that will state that the element is not present in the set. The time complexity of this method is also O(1) since it has a hash method inside (O(1)) and, possibly, a limited number of the modifications in the bit array, each of which takes O(1) time.

  (The property of being able to delete an element from the set is a special characteristic that differentiates Counting Bloom Filters from usual Bloom Filters. In the classical Bloom Filter data structure, the insert method would set all the counters obtained by our set of hash functions to 1. In other words, each of the bits will be 0 if none of the hashed sets contained its index when and 1 otherwise. Thus, it is impossible to delete an element from the set in Bloom Filters since we can't tell which of the k bits this element is mapped to should be set to 0. If we were about to set all k bits to 0, we wouldn't know whether we accidentally removed any other elements mapped to this bit.)

  It is worth noticing that regardless of the order/number of operations within the CBF class, all of the bit counters will have a non-negative integer value. The first and third methods mentioned, hashing and search, don't influence counters' values. The insertion method will only increase counters' values since it will add 1s to the bits created by our hash functions, which means that our property will be maintained after the insert method. The delete method will only be applied when we are sure that the item is present in the set, meaning that corresponding to an item k bits in the bit array have a value of at least 1. Subtracting 1, thus, will still keep our counters have the property described above. Since all of our elements in the bit array were set to 0 initially, we are sure that all the counters would have a non-negative integer value at any point of our CBF. Counters' values would then represent the number of items that contain this bit during hashing since insertion makes us increase a particular bit by 1, while deletion decreases a bit by 1.

## Plagiarism detector

In order to find the level of plagiarism between two texts, I would find the same phrases that exist in both texts. Accessing the level of plagiarism by the number of matching words is not an efficient strategy since there are obviously going to be intersections that did not happen because of the plagiarism probability but rather being frequent words we use in everyday life. However, if a phrase consisting of 5 words was repeated in both texts, I would classify this as plagiarism. I would define the percentage of plagiarism as the fraction of the number of repeating phrases divided by the total number of phrases in the text. This would give us an accurate representation of how texts are similar to each other. 

An efficient way to do this would be through rolling hashing. First of all, we would create a CountingBloomFilter with a size of the number of phrases in the 1st text (as I mentioned earlier, I define a phrase as a set of 5 consecutive words classified as a phrase). Then, I use a window of 5 words – it starts from 1st to 5th, then it transfers from 2nd to 6th, from 3rd to 7th, etc. All of these phrases are going to be added to our CountingBloomFIlter in order for us to be able to find similarities between the first and second text in the future. The way I add elements, however, is not simply calculating the hash values of each element separately. Because our phrases constantly have one word added and one word removed at a time, a better strategy would be to see our polynomial hash function for the hash functions and try to establish the connection between the old and new hash values. Assuming that the current “base” is p, we will be able to do an efficient calculations that are implemented in the code. After finishing adding all of the phrases from the first text, I will use the same strategy to search for all the phrases from the second text in the CBF. If the program finds a match, it will need to make sure that it is actually plagiarism and not a False Positive by searching up the entire first text. I will start looking for the first word from the “suspicious” phrase from the second text, and in case I find this word, I will compare the phrases, word by word, to verify whether it is actual plagiarism or we just ended up with a collision. 

## Conclusion

As we could see from our implementation, all three pairs of texts we test don't have plagiarism (as they shouldn't), with the probability of False Positives decreasing as the false-positive rate of our Counting Bloom Filter decreases (our filter starts to identify duplicates more accurately). Usage of CBFs is justified since the runtime of all our methods is dependent only on the false-positive rate of our CBF (rather than on the size of the input).
